================================================================================
LoRA Matrix Size Monitor Log
Started at: 2025-09-29 15:31:37.944605
================================================================================

Round 0 - Client unknown - 2025-09-29 15:31:37.944970
------------------------------------------------------------
LoRA A Matrices:
  layer.0.attention.self.query:
    Shape: (8, 768)
    Size: 6144 parameters
    Dtype: torch.float32
    Requires Grad: True
  layer.0.attention.self.value:
    Shape: (8, 768)
    Size: 6144 parameters
    Dtype: torch.float32
    Requires Grad: True
  layer.1.attention.self.query:
    Shape: (8, 768)
    Size: 6144 parameters
    Dtype: torch.float32
    Requires Grad: True
  layer.1.attention.self.value:
    Shape: (8, 768)
    Size: 6144 parameters
    Dtype: torch.float32
    Requires Grad: True
  Total LoRA A Parameters: 24576

LoRA B Matrices:
  layer.0.attention.self.query:
    Shape: (768, 8)
    Size: 6144 parameters
    Dtype: torch.float32
    Requires Grad: True
  layer.0.attention.self.value:
    Shape: (768, 8)
    Size: 6144 parameters
    Dtype: torch.float32
    Requires Grad: True
  layer.1.attention.self.query:
    Shape: (768, 8)
    Size: 6144 parameters
    Dtype: torch.float32
    Requires Grad: True
  layer.1.attention.self.value:
    Shape: (768, 8)
    Size: 6144 parameters
    Dtype: torch.float32
    Requires Grad: True
  Total LoRA B Parameters: 24576
  Total LoRA Parameters (A+B): 49152

================================================================================

